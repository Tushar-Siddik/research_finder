{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67082d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "research_finder\n",
      "├── __pycache__\n",
      "│   └── config.cpython-311.pyc\n",
      "├── cache\n",
      "│   └── 817de8751c24a3553f9a3f250d447a20.json\n",
      "├── config.py\n",
      "├── main.py\n",
      "├── requirements.txt\n",
      "├── research_finder\n",
      "│   ├── __init__.py\n",
      "│   ├── __pycache__\n",
      "│   │   ├── __init__.cpython-311.pyc\n",
      "│   │   ├── aggregator.cpython-311.pyc\n",
      "│   │   ├── cache.cpython-311.pyc\n",
      "│   │   ├── exporter.cpython-311.pyc\n",
      "│   │   └── utils.cpython-311.pyc\n",
      "│   ├── aggregator.py\n",
      "│   ├── cache.py\n",
      "│   ├── exporter.py\n",
      "│   ├── searchers\n",
      "│   │   ├── __init__.py\n",
      "│   │   ├── __pycache__\n",
      "│   │   │   ├── __init__.cpython-311.pyc\n",
      "│   │   │   ├── arxiv.cpython-311.pyc\n",
      "│   │   │   ├── base_searcher.cpython-311.pyc\n",
      "│   │   │   ├── crossref.cpython-311.pyc\n",
      "│   │   │   ├── google_scholar.cpython-311.pyc\n",
      "│   │   │   ├── openalex.cpython-311.pyc\n",
      "│   │   │   ├── pubmed.cpython-311.pyc\n",
      "│   │   │   └── semantic_scholar.cpython-311.pyc\n",
      "│   │   ├── arxiv.py\n",
      "│   │   ├── base_searcher.py\n",
      "│   │   ├── crossref.py\n",
      "│   │   ├── google_scholar.py\n",
      "│   │   ├── openalex.py\n",
      "│   │   ├── pubmed.py\n",
      "│   │   └── semantic_scholar.py\n",
      "│   └── utils.py\n",
      "└── search_tool.ipynb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def tree(dir_path: str, prefix: str = \"\"):\n",
    "    entries = sorted(os.listdir(dir_path))\n",
    "    entries = [e for e in entries if not e.startswith('.')]  # Skip hidden files\n",
    "    entries_count = len(entries)\n",
    "\n",
    "    for index, entry in enumerate(entries):\n",
    "        path = os.path.join(dir_path, entry)\n",
    "        connector = \"└── \" if index == entries_count - 1 else \"├── \"\n",
    "        print(prefix + connector + entry)\n",
    "        if os.path.isdir(path):\n",
    "            extension = \"    \" if index == entries_count - 1 else \"│   \"\n",
    "            tree(path, prefix + extension)\n",
    "\n",
    "# Example usage\n",
    "cwd = os.getcwd()\n",
    "print(os.path.basename(cwd))\n",
    "tree(cwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b0d61a",
   "metadata": {},
   "source": [
    "research_finder/\n",
    "├── requirements.txt\n",
    "├── main.py\n",
    "└── research_finder/\n",
    "    ├── __init__.py\n",
    "    ├── aggregator.py\n",
    "    ├── exporter.py\n",
    "    └── searchers/\n",
    "        ├── __init__.py\n",
    "        ├── base_searcher.py\n",
    "        ├── semantic_scholar.py\n",
    "        ├── arxiv.py\n",
    "        └── google_scholar.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13be11",
   "metadata": {},
   "source": [
    "Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c24ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: feedparser in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (6.0.12)\n",
      "Requirement already satisfied: scholarly in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.7.11)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2024.6.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from feedparser) (1.0.0)\n",
      "Requirement already satisfied: arrow in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scholarly) (1.3.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scholarly) (4.12.3)\n",
      "Requirement already satisfied: bibtexparser in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scholarly) (1.4.3)\n",
      "Requirement already satisfied: deprecated in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scholarly) (1.2.18)\n",
      "Requirement already satisfied: fake-useragent in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scholarly) (2.2.0)\n",
      "Requirement already satisfied: free-proxy in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scholarly) (1.1.3)\n",
      "Requirement already satisfied: httpx in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scholarly) (0.28.1)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scholarly) (1.0.1)\n",
      "Requirement already satisfied: selenium in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scholarly) (4.28.1)\n",
      "Requirement already satisfied: sphinx-rtd-theme in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scholarly) (3.0.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scholarly) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from arrow->scholarly) (2.9.0.20241206)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4->scholarly) (2.5)\n",
      "Requirement already satisfied: pyparsing>=2.0.3 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from bibtexparser->scholarly) (3.1.2)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from deprecated->scholarly) (1.16.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from free-proxy->scholarly) (5.3.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx->scholarly) (4.9.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx->scholarly) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx->scholarly) (0.14.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio->httpx->scholarly) (1.3.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests[socks]->scholarly) (1.7.1)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium->scholarly) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium->scholarly) (0.12.1)\n",
      "Requirement already satisfied: websocket-client~=1.8 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from selenium->scholarly) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (25.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (2.4.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (1.3.0.post0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio~=0.17->selenium->scholarly) (1.16.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from trio-websocket~=0.9->selenium->scholarly) (1.2.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium->scholarly) (2.22)\n",
      "Requirement already satisfied: sphinx<9,>=6 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx-rtd-theme->scholarly) (8.2.3)\n",
      "Requirement already satisfied: docutils<0.22,>0.18 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx-rtd-theme->scholarly) (0.21.2)\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx-rtd-theme->scholarly) (4.1)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.1.0)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.0.0)\n",
      "Requirement already satisfied: Jinja2>=3.1 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.4)\n",
      "Requirement already satisfied: Pygments>=2.17 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.18.0)\n",
      "Requirement already satisfied: snowballstemmer>=2.2 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.0.1)\n",
      "Requirement already satisfied: babel>=2.13 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.17.0)\n",
      "Requirement already satisfied: alabaster>=0.7.14 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.0.0)\n",
      "Requirement already satisfied: imagesize>=1.3 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (1.4.1)\n",
      "Requirement already satisfied: roman-numerals-py>=1.0.0 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (3.1.0)\n",
      "Requirement already satisfied: packaging>=23.0 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (24.1)\n",
      "Requirement already satisfied: colorama>=0.4.6 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sphinx<9,>=6->sphinx-rtd-theme->scholarly) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\srtos\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Jinja2>=3.1->sphinx<9,>=6->sphinx-rtd-theme->scholarly) (2.1.5)\n",
      "Libraries installed and logging configured.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries if you haven't already\n",
    "# Note: In a Jupyter environment, you can run shell commands with '!'\n",
    "!pip install requests pandas feedparser scholarly\n",
    "\n",
    "# Standard library imports\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Setup basic logging to see output from our classes\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "print(\"Libraries installed and logging configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1d1c6e",
   "metadata": {},
   "source": [
    "The Base Class (BaseSearcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8104bec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseSearcher class defined.\n"
     ]
    }
   ],
   "source": [
    "# research_finder/searchers/base_searcher.py\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class BaseSearcher(ABC):\n",
    "    \"\"\"Abstract base class for all article searchers.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.results: List[Dict[str, Any]] = []\n",
    "\n",
    "    @abstractmethod\n",
    "    def search(self, query: str, limit: int) -> None:\n",
    "        \"\"\"\n",
    "        Performs a search and populates the self.results list.\n",
    "        Each result should be a dictionary.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_results(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Returns the list of standardized results.\"\"\"\n",
    "        return self.results\n",
    "\n",
    "    def clear_results(self) -> None:\n",
    "        \"\"\"Clears the stored results.\"\"\"\n",
    "        self.results = []\n",
    "        \n",
    "print(\"BaseSearcher class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3d0b8c",
   "metadata": {},
   "source": [
    "Semantic Scholar Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7a4976e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemanticScholarSearcher class defined.\n"
     ]
    }
   ],
   "source": [
    "# research_finder/searchers/semantic_scholar.py\n",
    "\n",
    "import requests\n",
    "from IPython.display import display, Markdown # For nice output in notebooks\n",
    "\n",
    "class SemanticScholarSearcher(BaseSearcher):\n",
    "    \"\"\"Searcher for the Semantic Scholar API.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Semantic Scholar\")\n",
    "        self.logger = logging.getLogger(self.name)\n",
    "\n",
    "    def search(self, query: str, limit: int = 10) -> None:\n",
    "        self.logger.info(f\"Searching for: '{query}' with limit {limit}\")\n",
    "        self.clear_results()\n",
    "        params = {\n",
    "            'query': query,\n",
    "            'limit': limit,\n",
    "            'fields': 'title,authors,year,abstract,url,citationCount,tldr'\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(self.BASE_URL, params=params)\n",
    "            response.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)\n",
    "            data = response.json()\n",
    "            \n",
    "            for item in data.get('data', []):\n",
    "                authors = [author.get('name') for author in item.get('authors', [])]\n",
    "                abstract = item.get('tldr', {}).get('text') or item.get('abstract')\n",
    "\n",
    "                paper = {\n",
    "                    'Title': item.get('title'),\n",
    "                    'Authors': ', '.join(authors),\n",
    "                    'Year': item.get('year'),\n",
    "                    'Abstract': abstract,\n",
    "                    'URL': item.get('url'),\n",
    "                    'Source': self.name\n",
    "                }\n",
    "                self.results.append(paper)\n",
    "            self.logger.info(f\"Found {len(self.results)} papers.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.error(f\"API request failed: {e}\")\n",
    "\n",
    "print(\"SemanticScholarSearcher class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de936dd8",
   "metadata": {},
   "source": [
    "arXiv Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24fb467d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArxivSearcher class defined.\n"
     ]
    }
   ],
   "source": [
    "# research_finder/searchers/arxiv.py\n",
    "\n",
    "import requests\n",
    "import feedparser\n",
    "\n",
    "class ArxivSearcher(BaseSearcher):\n",
    "    \"\"\"Searcher for the arXiv API.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"http://export.arxiv.org/api/query\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"arXiv\")\n",
    "        self.logger = logging.getLogger(self.name)\n",
    "\n",
    "    def search(self, query: str, limit: int = 10) -> None:\n",
    "        self.logger.info(f\"Searching for: '{query}' with limit {limit}\")\n",
    "        self.clear_results()\n",
    "        params = {\n",
    "            'search_query': f'all:\"{query}\"',\n",
    "            'start': 0,\n",
    "            'max_results': limit\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(self.BASE_URL, params=params)\n",
    "            response.raise_for_status()\n",
    "            feed = feedparser.parse(response.content)\n",
    "\n",
    "            for entry in feed.entries:\n",
    "                authors = [author.name for author in entry.authors]\n",
    "                paper = {\n",
    "                    'Title': entry.title,\n",
    "                    'Authors': ', '.join(authors),\n",
    "                    'Year': entry.published.split('-')[0],\n",
    "                    'Abstract': entry.summary,\n",
    "                    'URL': entry.link,\n",
    "                    'Source': self.name\n",
    "                }\n",
    "                self.results.append(paper)\n",
    "            self.logger.info(f\"Found {len(self.results)} papers.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.error(f\"API request failed: {e}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to parse arXiv response: {e}\")\n",
    "\n",
    "print(\"ArxivSearcher class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bed9a57",
   "metadata": {},
   "source": [
    "The Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9525bac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregator class defined.\n"
     ]
    }
   ],
   "source": [
    "# research_finder/aggregator.py\n",
    "\n",
    "class Aggregator:\n",
    "    \"\"\"Aggregates results from multiple searchers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.searchers: List[BaseSearcher] = []\n",
    "        self.logger = logging.getLogger(\"Aggregator\")\n",
    "\n",
    "    def add_searcher(self, searcher: BaseSearcher) -> None:\n",
    "        \"\"\"Adds a searcher instance to the list.\"\"\"\n",
    "        if isinstance(searcher, BaseSearcher):\n",
    "            self.searchers.append(searcher)\n",
    "            self.logger.info(f\"Added searcher: {searcher.name}\")\n",
    "        else:\n",
    "            self.logger.error(f\"Failed to add searcher: {searcher} is not a valid BaseSearcher instance.\")\n",
    "\n",
    "    def run_all_searches(self, query: str, limit: int) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Runs the search query on all added searchers and returns combined results.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"--- Starting search for '{query}' ---\")\n",
    "        all_results = []\n",
    "        for searcher in self.searchers:\n",
    "            try:\n",
    "                searcher.search(query, limit)\n",
    "                all_results.extend(searcher.get_results())\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"An error occurred with searcher '{searcher.name}': {e}\")\n",
    "        \n",
    "        self.logger.info(f\"--- Search complete. Total results found: {len(all_results)} ---\")\n",
    "        return all_results\n",
    "\n",
    "print(\"Aggregator class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54917528",
   "metadata": {},
   "source": [
    "The Exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdd63a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporter class defined.\n"
     ]
    }
   ],
   "source": [
    "# research_finder/exporter.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "class Exporter:\n",
    "    \"\"\"Handles exporting data to various formats.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(\"Exporter\")\n",
    "\n",
    "    def to_csv(self, data: List[Dict[str, Any]], filename: str) -> None:\n",
    "        \"\"\"Exports a list of dictionaries to a CSV file.\"\"\"\n",
    "        if not data:\n",
    "            self.logger.warning(\"No data provided to export.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            df = pd.DataFrame(data)\n",
    "            # Ensure a consistent column order\n",
    "            df = df[['Title', 'Authors', 'Year', 'Source', 'URL', 'Abstract']]\n",
    "            df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            self.logger.info(f\"Successfully exported {len(data)} results to {filename}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to export to CSV: {e}\")\n",
    "\n",
    "print(\"Exporter class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0435132",
   "metadata": {},
   "source": [
    "Main Execution and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8804bf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 16:23:27,907 - Aggregator - INFO - Added searcher: Semantic Scholar\n",
      "2025-09-30 16:23:27,907 - Aggregator - INFO - Added searcher: arXiv\n",
      "2025-09-30 16:23:27,908 - Aggregator - INFO - --- Starting search for 'quantum computing' ---\n",
      "2025-09-30 16:23:27,908 - Semantic Scholar - INFO - Searching for: 'quantum computing' with limit 5\n",
      "2025-09-30 16:23:29,690 - Aggregator - ERROR - An error occurred with searcher 'Semantic Scholar': 'NoneType' object has no attribute 'get'\n",
      "2025-09-30 16:23:29,690 - arXiv - INFO - Searching for: 'quantum computing' with limit 5\n",
      "2025-09-30 16:23:30,444 - arXiv - INFO - Found 5 papers.\n",
      "2025-09-30 16:23:30,445 - Aggregator - INFO - --- Search complete. Total results found: 5 ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample of Results Found ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Year</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>URL</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pulse controlled noise suppressed quantum comp...</td>\n",
       "      <td>Lu-Ming Duan, Guang-Can Guo</td>\n",
       "      <td>1998</td>\n",
       "      <td>To make arbitrarily accurate quantum computati...</td>\n",
       "      <td>http://arxiv.org/abs/quant-ph/9807072v1</td>\n",
       "      <td>arXiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Unconventional Quantum Computing Devices</td>\n",
       "      <td>Seth Lloyd</td>\n",
       "      <td>2000</td>\n",
       "      <td>This paper investigates a variety of unconvent...</td>\n",
       "      <td>http://arxiv.org/abs/quant-ph/0003151v1</td>\n",
       "      <td>arXiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Photonic Quantum Computers</td>\n",
       "      <td>M. AbuGhanem</td>\n",
       "      <td>2024</td>\n",
       "      <td>In the pursuit of scalable and fault-tolerant ...</td>\n",
       "      <td>http://arxiv.org/abs/2409.08229v1</td>\n",
       "      <td>arXiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quantum computer and its quasiclassical model</td>\n",
       "      <td>Timur F. Kamalov</td>\n",
       "      <td>2001</td>\n",
       "      <td>Could the theories with hidden variables be em...</td>\n",
       "      <td>http://arxiv.org/abs/quant-ph/0109152v1</td>\n",
       "      <td>arXiv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantum Computing for Multi Period Asset Alloc...</td>\n",
       "      <td>Queenie Sun, Nicholas Grablevsky, Huaizhang De...</td>\n",
       "      <td>2024</td>\n",
       "      <td>Portfolio construction has been a long-standin...</td>\n",
       "      <td>http://arxiv.org/abs/2410.11997v1</td>\n",
       "      <td>arXiv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Pulse controlled noise suppressed quantum comp...   \n",
       "1           Unconventional Quantum Computing Devices   \n",
       "2                         Photonic Quantum Computers   \n",
       "3      Quantum computer and its quasiclassical model   \n",
       "4  Quantum Computing for Multi Period Asset Alloc...   \n",
       "\n",
       "                                             Authors  Year  \\\n",
       "0                        Lu-Ming Duan, Guang-Can Guo  1998   \n",
       "1                                         Seth Lloyd  2000   \n",
       "2                                       M. AbuGhanem  2024   \n",
       "3                                   Timur F. Kamalov  2001   \n",
       "4  Queenie Sun, Nicholas Grablevsky, Huaizhang De...  2024   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  To make arbitrarily accurate quantum computati...   \n",
       "1  This paper investigates a variety of unconvent...   \n",
       "2  In the pursuit of scalable and fault-tolerant ...   \n",
       "3  Could the theories with hidden variables be em...   \n",
       "4  Portfolio construction has been a long-standin...   \n",
       "\n",
       "                                       URL Source  \n",
       "0  http://arxiv.org/abs/quant-ph/9807072v1  arXiv  \n",
       "1  http://arxiv.org/abs/quant-ph/0003151v1  arXiv  \n",
       "2        http://arxiv.org/abs/2409.08229v1  arXiv  \n",
       "3  http://arxiv.org/abs/quant-ph/0109152v1  arXiv  \n",
       "4        http://arxiv.org/abs/2410.11997v1  arXiv  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 16:23:30,466 - Exporter - INFO - Successfully exported 5 results to notebook_test_results.csv\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution Logic ---\n",
    "\n",
    "# 1. Define your search parameters\n",
    "SEARCH_QUERY = \"quantum computing\"\n",
    "OUTPUT_FILE = \"notebook_test_results.csv\"\n",
    "RESULTS_PER_SOURCE = 5\n",
    "\n",
    "# 2. Initialize the components\n",
    "aggregator = Aggregator()\n",
    "exporter = Exporter()\n",
    "\n",
    "# 3. Add the searchers you want to use\n",
    "# To debug, you can comment out one of these lines.\n",
    "aggregator.add_searcher(SemanticScholarSearcher())\n",
    "aggregator.add_searcher(ArxivSearcher())\n",
    "\n",
    "# 4. Run the searches\n",
    "all_articles = aggregator.run_all_searches(SEARCH_QUERY, RESULTS_PER_SOURCE)\n",
    "\n",
    "# 5. Display a sample of the results in the notebook\n",
    "if all_articles:\n",
    "    print(\"\\n--- Sample of Results Found ---\")\n",
    "    df_display = pd.DataFrame(all_articles)\n",
    "    display(df_display.head()) # Shows first 5 rows\n",
    "    \n",
    "    # 6. Export all results to the CSV file\n",
    "    exporter.to_csv(all_articles, OUTPUT_FILE)\n",
    "else:\n",
    "    print(\"No articles found to export.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c7a4a9",
   "metadata": {},
   "source": [
    "I am building a tool in Python that will search for research articles using key words in various vendors and websites. In return, it will provide 'Title', 'Authors', 'Year', 'Venue', 'Source', 'Citation Count', 'DOI', 'License Type', 'URL', 'APA 7 Reference' of the matched articles. The tool is capable to export this in a csv file upon request. The user can manually enter the key words and also can manually select a vendor from available vendors list. Here is the directory tree for your better understanding.\n",
    "\n",
    "research_finder\n",
    "├── config.py\n",
    "├── main.py\n",
    "├── requirements.txt\n",
    "├── research_finder\n",
    "│   ├── __init__.py\n",
    "│   ├── aggregator.py\n",
    "│   ├── cache.py\n",
    "│   ├── exporter.py\n",
    "│   ├── searchers\n",
    "│   │   ├── __init__.py\n",
    "│   │   ├── arxiv.py\n",
    "│   │   ├── base_searcher.py\n",
    "│   │   ├── crossref.py\n",
    "│   │   ├── google_scholar.py\n",
    "│   │   ├── openalex.py\n",
    "│   │   ├── pubmed.py\n",
    "│   │   └── semantic_scholar.py\n",
    "│   └── utils.py\n",
    "└── search_tool.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ad10dc",
   "metadata": {},
   "source": [
    "i am providing you all the codes one by one:\n",
    "\n",
    "requirements.txt:\n",
    "requests\n",
    "pandas\n",
    "feedparser\n",
    "scholarly\n",
    "\n",
    "\n",
    "main.py:\n",
    "import logging\n",
    "from research_finder.aggregator import Aggregator\n",
    "from research_finder.exporter import Exporter\n",
    "from research_finder.searchers.semantic_scholar import SemanticScholarSearcher\n",
    "from research_finder.searchers.arxiv import ArxivSearcher\n",
    "\n",
    "# We will handle the optional Google Scholar import here\n",
    "try:\n",
    "    from research_finder.searchers.google_scholar import GoogleScholarSearcher\n",
    "    GOOGLE_SCHOLAR_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GOOGLE_SCHOLAR_AVAILABLE = False\n",
    "    print(\"Warning: 'scholarly' library not found. Google Scholar will not be an option.\")\n",
    "    print(\"To enable it, run: pip install scholarly\")\n",
    "\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configure basic logging for the application.\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "        handlers=[logging.StreamHandler()]\n",
    "    )\n",
    "\n",
    "def get_user_input():\n",
    "    \"\"\"Gets search parameters from the user via command-line prompts.\"\"\"\n",
    "    print(\"\\n--- Research Article Finder ---\")\n",
    "    query = input(\"Enter keywords to search for: \").strip()\n",
    "    if not query:\n",
    "        print(\"Search query cannot be empty. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    output_file = input(\"Enter output CSV filename (e.g., results.csv): \").strip()\n",
    "    if not output_file:\n",
    "        output_file = \"search_results.csv\"\n",
    "    if not output_file.endswith('.csv'):\n",
    "        output_file += '.csv'\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            limit_str = input(\"Enter max results per source (e.g., 10): \").strip()\n",
    "            limit = int(limit_str)\n",
    "            if limit > 0:\n",
    "                break\n",
    "            else:\n",
    "                print(\"Please enter a positive number.\")\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "            \n",
    "    return query, output_file, limit\n",
    "\n",
    "def get_searcher_selection():\n",
    "    \"\"\"\n",
    "    Displays a menu of available searchers and gets the user's selection.\n",
    "    \"\"\"\n",
    "    # Define the list of available searchers\n",
    "    # Each item is a tuple: (Display Name, Searcher Class)\n",
    "    available_searchers = [\n",
    "        (\"Semantic Scholar\", SemanticScholarSearcher),\n",
    "        (\"arXiv\", ArxivSearcher)\n",
    "    ]\n",
    "    if GOOGLE_SCHOLAR_AVAILABLE:\n",
    "        available_searchers.append((\"Google Scholar (Unreliable)\", GoogleScholarSearcher))\n",
    "\n",
    "    print(\"\\n--- Select Search Vendors ---\")\n",
    "    for i, (name, _) in enumerate(available_searchers, 1):\n",
    "        print(f\"  {i}. {name}\")\n",
    "    \n",
    "    while True:\n",
    "        choice_str = input(f\"Enter vendor numbers to use (e.g., 1,2) or press Enter for all: \").strip()\n",
    "        \n",
    "        # If user presses Enter, select all\n",
    "        if not choice_str:\n",
    "            return [searcher_class for (_, searcher_class) in available_searchers]\n",
    "\n",
    "        try:\n",
    "            # Parse comma-separated numbers\n",
    "            chosen_indices = [int(num.strip()) for num in choice_str.split(',')]\n",
    "            selected_searchers = []\n",
    "            \n",
    "            # Validate choices\n",
    "            for index in chosen_indices:\n",
    "                if 1 <= index <= len(available_searchers):\n",
    "                    selected_searchers.append(available_searchers[index - 1][1])\n",
    "                else:\n",
    "                    raise ValueError(f\"Invalid number: {index}\")\n",
    "            \n",
    "            if not selected_searchers:\n",
    "                print(\"No valid vendors selected. Please try again.\")\n",
    "                continue\n",
    "\n",
    "            return selected_searchers\n",
    "\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"Invalid input. Please enter numbers separated by commas (e.g., 1,3).\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the research finder tool.\"\"\"\n",
    "    setup_logging()\n",
    "    logger = logging.getLogger(\"Main\")\n",
    "\n",
    "    # 1. Get user input for the search\n",
    "    query, output_file, limit = get_user_input()\n",
    "\n",
    "    # 2. Get user's choice of search vendors\n",
    "    selected_searcher_classes = get_searcher_selection()\n",
    "\n",
    "    # 3. Initialize Components\n",
    "    aggregator = Aggregator()\n",
    "    exporter = Exporter()\n",
    "\n",
    "    # 4. Instantiate and add the selected searchers to the Aggregator\n",
    "    for searcher_class in selected_searcher_classes:\n",
    "        try:\n",
    "            aggregator.add_searcher(searcher_class())\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Could not initialize searcher {searcher_class.__name__}: {e}\")\n",
    "\n",
    "    # 5. Run Searches and Get Results\n",
    "    all_articles = aggregator.run_all_searches(query, limit)\n",
    "\n",
    "    # 6. Export Results\n",
    "    if all_articles:\n",
    "        exporter.to_csv(all_articles, output_file)\n",
    "    else:\n",
    "        logger.info(\"No articles found to export.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    ".gitignore:\n",
    "# Byte-compiled / optimized / DLL files\n",
    "__pycache__/\n",
    "*.py[cod]\n",
    "*$py.class\n",
    "\n",
    "# C extensions\n",
    "*.so\n",
    "\n",
    "# Distribution / packaging\n",
    ".Python\n",
    "build/\n",
    "develop-eggs/\n",
    "dist/\n",
    "downloads/\n",
    "eggs/\n",
    ".eggs/\n",
    "lib/\n",
    "lib64/\n",
    "parts/\n",
    "sdist/\n",
    "var/\n",
    "wheels/\n",
    "*.egg-info/\n",
    ".installed.cfg\n",
    "*.egg\n",
    "MANIFEST\n",
    "\n",
    "# PyInstaller\n",
    "*.manifest\n",
    "*.spec\n",
    "\n",
    "# Installer logs\n",
    "pip-log.txt\n",
    "pip-delete-this-directory.txt\n",
    "\n",
    "# Unit test / coverage reports\n",
    "htmlcov/\n",
    ".tox/\n",
    ".coverage\n",
    ".coverage.*\n",
    ".cache\n",
    "nosetests.xml\n",
    "coverage.xml\n",
    "*.cover\n",
    ".hypothesis/\n",
    ".pytest_cache/\n",
    "\n",
    "# Jupyter Notebook\n",
    ".ipynb_checkpoints\n",
    "\n",
    "# pyenv\n",
    ".python-version\n",
    "\n",
    "# Environments\n",
    ".env\n",
    ".venv\n",
    "env/\n",
    "venv/\n",
    "ENV/\n",
    "env.bak/\n",
    "venv.bak/\n",
    "\n",
    "# IDEs\n",
    ".vscode/\n",
    ".idea/\n",
    "*.swp\n",
    "*.swo\n",
    "\n",
    "# Project specific output files\n",
    "*.csv\n",
    "*.json\n",
    "!requirements.txt\n",
    "\n",
    ".env:\n",
    "S2_API_KEY=\"your_actual_api_key_goes_here\"\n",
    "\n",
    "\n",
    "utils.py:\n",
    "def format_apa7(paper: dict) -> str:\n",
    "    \"\"\"\n",
    "    Formats a paper dictionary into a basic APA-7 style reference string.\n",
    "    Note: This is an approximation and may not cover all edge cases.\n",
    "    \"\"\"\n",
    "    # 1. Format Authors\n",
    "    authors_list = [a.strip() for a in paper.get('Authors', '').split(',') if a.strip()]\n",
    "    if not authors_list:\n",
    "        author_str = \"n.a.\"\n",
    "    else:\n",
    "        formatted_authors = []\n",
    "        for author in authors_list:\n",
    "            parts = author.split()\n",
    "            if len(parts) == 0:\n",
    "                continue\n",
    "            # Assumes \"Lastname Firstname\" or \"Lastname, F.\"\n",
    "            last_name = parts[-1]\n",
    "            initials = \"\".join([p[0] + \".\" for p in parts[:-1]])\n",
    "            formatted_authors.append(f\"{last_name}, {initials}\")\n",
    "        \n",
    "        if len(formatted_authors) == 1:\n",
    "            author_str = formatted_authors[0]\n",
    "        elif len(formatted_authors) <= 20:\n",
    "            author_str = \", \".join(formatted_authors[:-1]) + \", & \" + formatted_authors[-1]\n",
    "        else: # APA 7 rule for >20 authors\n",
    "            author_str = formatted_authors[0] + \", et al.\"\n",
    "\n",
    "    # 2. Get Year\n",
    "    year = paper.get('Year', 'n.d.')\n",
    "    year_str = f\"({year}).\"\n",
    "\n",
    "    # 3. Get Title\n",
    "    title = paper.get('Title', '')\n",
    "    \n",
    "    # 4. Get Source/Venue and construct reference\n",
    "    source = paper.get('Source')\n",
    "    venue = paper.get('Venue', '')\n",
    "    doi = paper.get('DOI', '')\n",
    "    url = paper.get('URL', '')\n",
    "\n",
    "    if source == 'arXiv':\n",
    "        # Preprint format\n",
    "        ref = f\"{author_str} {year_str} *{title}* [Preprint]. arXiv.\"\n",
    "        if url:\n",
    "            ref += f\" {url}\"\n",
    "    else:\n",
    "        # Journal article format\n",
    "        ref = f\"{author_str} {year_str} {title}.\"\n",
    "        if venue:\n",
    "            ref += f\" *{venue}*.\"\n",
    "        \n",
    "        # Add DOI or URL\n",
    "        if doi:\n",
    "            ref += f\" https://doi.org/{doi}\"\n",
    "        elif url:\n",
    "            ref += f\" {url}\"\n",
    "            \n",
    "    return ref.strip()\n",
    "\n",
    "\n",
    "\n",
    "exporter.py:\n",
    "import pandas as pd\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from .utils import format_apa7\n",
    "\n",
    "class Exporter:\n",
    "    \"\"\"Handles exporting data to various formats.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(\"Exporter\")\n",
    "\n",
    "    def to_csv(self, data: List[Dict[str, Any]], filename: str) -> None:\n",
    "        \"\"\"Exports a list of dictionaries to a CSV file with a fixed set of columns.\"\"\"\n",
    "        if not data:\n",
    "            self.logger.warning(\"No data provided to export.\")\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            # 1. Generate APA 7 reference for each paper\n",
    "            for paper in data:\n",
    "                paper['APA 7 Reference'] = format_apa7(paper)\n",
    "\n",
    "            # 2. Define the fixed, final order of columns for the output CSV\n",
    "            final_columns = [\n",
    "                'Title', 'Authors', 'Year', 'Venue', 'Source', 'Citation Count', 'DOI', 'License Type',\n",
    "                'URL', 'APA 7 Reference'\n",
    "            ]\n",
    "\n",
    "            # 3. Create the DataFrame\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # 4. Ensure all desired columns exist in the DataFrame before reordering\n",
    "            # This prevents errors if a searcher doesn't provide a specific field\n",
    "            for col in final_columns:\n",
    "                if col not in df.columns:\n",
    "                    df[col] = '' # Add missing columns as empty strings\n",
    "\n",
    "            # 5. Reorder the DataFrame and save to CSV\n",
    "            final_df = df[final_columns]\n",
    "            final_df.to_csv(filename, index=False, encoding='utf-8')\n",
    "            \n",
    "            self.logger.info(f\"Successfully exported {len(data)} results to {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to export to CSV: {e}\")\n",
    "\n",
    "\n",
    "aggregator.py:\n",
    "import logging\n",
    "from typing import List\n",
    "from .searchers.base_searcher import BaseSearcher\n",
    "\n",
    "class Aggregator:\n",
    "    \"\"\"Aggregates results from multiple searchers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.searchers: List[BaseSearcher] = []\n",
    "        self.logger = logging.getLogger(\"Aggregator\")\n",
    "\n",
    "    def add_searcher(self, searcher: BaseSearcher) -> None:\n",
    "        \"\"\"Adds a searcher instance to the list.\"\"\"\n",
    "        if isinstance(searcher, BaseSearcher):\n",
    "            self.searchers.append(searcher)\n",
    "            self.logger.info(f\"Added searcher: {searcher.name}\")\n",
    "        else:\n",
    "            self.logger.error(f\"Failed to add searcher: {searcher} is not a valid BaseSearcher instance.\")\n",
    "\n",
    "    def run_all_searches(self, query: str, limit: int) -> List[dict]:\n",
    "        \"\"\"\n",
    "        Runs the search query on all added searchers and returns combined results.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"--- Starting search for '{query}' ---\")\n",
    "        all_results = []\n",
    "        for searcher in self.searchers:\n",
    "            try:\n",
    "                searcher.search(query, limit)\n",
    "                all_results.extend(searcher.get_results())\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"An error occurred with searcher '{searcher.name}': {e}\")\n",
    "        \n",
    "        self.logger.info(f\"--- Search complete. Total results found: {len(all_results)} ---\")\n",
    "        return all_results\n",
    "\n",
    "\n",
    "semantic_scholar.py:\n",
    "import requests\n",
    "import logging\n",
    "from .base_searcher import BaseSearcher\n",
    "\n",
    "class SemanticScholarSearcher(BaseSearcher):\n",
    "    \"\"\"Searcher for the Semantic Scholar API.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"Semantic Scholar\")\n",
    "        self.logger = logging.getLogger(self.name)\n",
    "\n",
    "    def search(self, query: str, limit: int = 10) -> None:\n",
    "        self.logger.info(f\"Searching for: '{query}' with limit {limit}\")\n",
    "        self.clear_results()\n",
    "        # UPDATED: Added 'openAccessPdf.license' to the fields to retrieve\n",
    "        params = {\n",
    "            'query': query,\n",
    "            'limit': limit,\n",
    "            'fields': 'title,authors,year,abstract,url,citationCount,tldr,doi,venue,openAccessPdf.license'\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(self.BASE_URL, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            for item in data.get('data', []):\n",
    "                authors = [author.get('name') for author in item.get('authors', [])]\n",
    "                abstract = item.get('tldr', {}).get('text') or item.get('abstract')\n",
    "                \n",
    "                # UPDATED: Extract license information\n",
    "                license_info = item.get('openAccessPdf', {}).get('license') or 'N/A'\n",
    "\n",
    "                paper = {\n",
    "                    'Title': item.get('title'),\n",
    "                    'Authors': ', '.join(authors),\n",
    "                    'Year': item.get('year'),\n",
    "                    # 'Abstract': abstract,\n",
    "                    'URL': item.get('url'),\n",
    "                    'Source': self.name,\n",
    "                    'Citation': item.get('citationCount', 0),\n",
    "                    'DOI': item.get('doi'),\n",
    "                    'Venue': item.get('venue'),\n",
    "                    'License Type': license_info\n",
    "                }\n",
    "                self.results.append(paper)\n",
    "            self.logger.info(f\"Found {len(self.results)} papers.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.error(f\"API request failed: {e}\")\n",
    "\n",
    "\n",
    "google_scholar.py:\n",
    "# research_finder/searchers/google_scholar.py\n",
    "\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "from .base_searcher import BaseSearcher\n",
    "\n",
    "try:\n",
    "    from scholarly import scholarly\n",
    "except ImportError:\n",
    "    scholarly = None\n",
    "\n",
    "class GoogleScholarSearcher(BaseSearcher):\n",
    "    \"\"\"Searcher for Google Scholar using the unofficial 'scholarly' library.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        if not scholarly:\n",
    "            raise ImportError(\"scholarly library not found. Install with 'pip install scholarly'\")\n",
    "        super().__init__(\"Google Scholar\")\n",
    "        self.logger = logging.getLogger(self.name)\n",
    "\n",
    "    def search(self, query: str, limit: int = 5) -> None:\n",
    "        self.logger.info(f\"Searching for: '{query}' with limit {limit}. (Caution: Unreliable)\")\n",
    "        self.clear_results()\n",
    "        try:\n",
    "            search_query = scholarly.search_pubs(query)\n",
    "            for i, pub in enumerate(search_query):\n",
    "                if i >= limit:\n",
    "                    break\n",
    "                \n",
    "                doi = None\n",
    "                url = pub.get('pub_url', '')\n",
    "                if 'doi.org/' in url:\n",
    "                    doi = url.split('doi.org/')[-1]\n",
    "\n",
    "                paper = {\n",
    "                    'Title': pub.get('bib', {}).get('title'),\n",
    "                    'Authors': pub.get('bib', {}).get('author', ''),\n",
    "                    'Year': pub.get('bib', {}).get('pub_year'),\n",
    "                    # 'Abstract': pub.get('bib', {}).get('abstract'),\n",
    "                    'URL': url,\n",
    "                    'Source': self.name,\n",
    "                    'Citation': pub.get('bib', {}).get('num_citations', 'N/A'),\n",
    "                    'DOI': doi,\n",
    "                    'Venue': pub.get('bib', {}).get('journal', ''),\n",
    "                    'License Type': 'N/A'\n",
    "                }\n",
    "                self.results.append(paper)\n",
    "                time.sleep(1)\n",
    "            self.logger.info(f\"Found {len(self.results)} papers.\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Search failed: {e}. This is common with Google Scholar.\")\n",
    "\n",
    "\n",
    "base_searchers.py:\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "class BaseSearcher(ABC):\n",
    "    \"\"\"Abstract base class for all article searchers.\"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.results: List[Dict[str, Any]] = []\n",
    "\n",
    "    @abstractmethod\n",
    "    def search(self, query: str, limit: int) -> None:\n",
    "        \"\"\"\n",
    "        Performs a search and populates the self.results list.\n",
    "        Each result should be a dictionary.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_results(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Returns the list of standardized results.\"\"\"\n",
    "        return self.results\n",
    "\n",
    "    def clear_results(self) -> None:\n",
    "        \"\"\"Clears the stored results.\"\"\"\n",
    "        self.results = []\n",
    "\n",
    "\n",
    "arxiv.py:\n",
    "import requests\n",
    "import feedparser\n",
    "import logging\n",
    "from .base_searcher import BaseSearcher\n",
    "\n",
    "class ArxivSearcher(BaseSearcher):\n",
    "    \"\"\"Searcher for the arXiv API.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"http://export.arxiv.org/api/query\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(\"arXiv\")\n",
    "        self.logger = logging.getLogger(self.name)\n",
    "\n",
    "    def search(self, query: str, limit: int = 10) -> None:\n",
    "        self.logger.info(f\"Searching for: '{query}' with limit {limit}\")\n",
    "        self.clear_results()\n",
    "        params = {\n",
    "            'search_query': f'all:\"{query}\"',\n",
    "            'start': 0,\n",
    "            'max_results': limit\n",
    "        }\n",
    "        try:\n",
    "            response = requests.get(self.BASE_URL, params=params)\n",
    "            response.raise_for_status()\n",
    "            feed = feedparser.parse(response.content)\n",
    "\n",
    "            for entry in feed.entries:\n",
    "                authors = [author.name for author in entry.authors]\n",
    "                arxiv_id = entry.id.split('/')[-1]\n",
    "                \n",
    "                # UPDATED: Extract license information\n",
    "                license_info = entry.get('rights', 'N/A')\n",
    "\n",
    "                paper = {\n",
    "                    'Title': entry.title,\n",
    "                    'Authors': ', '.join(authors),\n",
    "                    'Year': entry.published.split('-')[0],\n",
    "                    # 'Abstract': entry.summary,\n",
    "                    'URL': entry.link,\n",
    "                    'Source': self.name,\n",
    "                    'Citation': 'N/A',\n",
    "                    'DOI': arxiv_id,\n",
    "                    'Venue': 'arXiv',\n",
    "                    # ADDED: New fields\n",
    "                    'License Type': license_info\n",
    "                }\n",
    "                self.results.append(paper)\n",
    "            self.logger.info(f\"Found {len(self.results)} papers.\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            self.logger.error(f\"API request failed: {e}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to parse arXiv response: {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
